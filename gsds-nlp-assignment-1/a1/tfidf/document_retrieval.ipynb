{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/heejinchae/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import scipy as sp\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[\"I AM SAM\",\n",
    "\"I LIKE SAM I AM\",\n",
    "\"I LIKE GREEN EGGS AND HAM\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 (a) \n",
    "Create a term document matrix using simple word counts (number of times a word appears in a document) for all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_word_counts(courpus)->np.ndarray:\n",
    "    \"\"\" Create a term-document matrix using simple word counts\n",
    "\n",
    "    Arguments:\n",
    "    corpus-- list of sentences in the corpus\n",
    "\n",
    "    Return:\n",
    "    term_doc_matrix -- term-document matrix where each element is the number of times a word appears in a document. The order of words DOES NOT matter.\n",
    "    \"\"\"\n",
    "    term_doc_matrix=np.empty([3,8])\n",
    "    #### YOUR CODE HERE (~ 6 lines) ####\n",
    "    term_doc_matrix = np.zeros(term_doc_matrix.shape)\n",
    "    token_in_sentence = []\n",
    "    for sentence in courpus:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        token_in_sentence.append(tokens)\n",
    "    vocab = np.unique(\n",
    "        np.concatenate(token_in_sentence).ravel()\n",
    "        )\n",
    "    vocab.sort()\n",
    "    vocab_to_idx = {word:i for i,word in enumerate(vocab)}\n",
    "    \n",
    "    for i,tokens in enumerate(token_in_sentence):\n",
    "        for token in tokens:\n",
    "            term_doc_matrix[i][vocab_to_idx[token]] += 1\n",
    "            \n",
    "    #### YOUR CODE HERE ####\n",
    "    return term_doc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 2., 1., 1.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_word_counts(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 (b)\n",
    "Using the vector space model and cosine similarity find the closest document to the user query ”I LIKE EGGS”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_distance(a:np.ndarray, b:np.ndarray)->float:\n",
    "    return 1 - sp.spatial.distance.cosine(a, b)\n",
    "\n",
    "def find_the_closest_document(corpus, query: str)->str:\n",
    "    \"\"\" Find the closest document with query from corpus\n",
    "\n",
    "    Arguments:\n",
    "    corpus-- list of sentences in the corpus\n",
    "    query-- a string to compute cosine similarity with\n",
    "\n",
    "    Return:\n",
    "    closest_document --the string in the corpus which is the closest to the query\n",
    "    \"\"\"\n",
    "    closest_document=str()\n",
    "    #### YOUR CODE HERE ####\n",
    "    unique_token = set() \n",
    "    token_list = [] \n",
    "    for sentence in corpus:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        token_list.append(list(tokens))\n",
    "        unique_token.update(tokens)\n",
    "    unique_token_list = sorted(list(unique_token)) # header\n",
    "    token_to_id = {token:i for i,token in enumerate(unique_token_list)}\n",
    "\n",
    "    term_doc_matrix = simple_word_counts(corpus)\n",
    "    query_tokens = word_tokenize(query)\n",
    "    query_token_ids = [token_to_id[token] for token in query_tokens]\n",
    "    query_token_vecs = np.zeros([term_doc_matrix.shape[0],term_doc_matrix.shape[1]])\n",
    "    query_token_vecs[:,query_token_ids] += 1\n",
    "\n",
    "    term_doc_matrix = list(term_doc_matrix)\n",
    "    query_token_vecs = list(query_token_vecs)\n",
    "    score = 0 \n",
    "    ind = 0\n",
    "    for i,(cor,qu) in enumerate(zip(term_doc_matrix, query_token_vecs)):\n",
    "        if cos_distance(cor,qu) > score:\n",
    "            score = cos_distance(cor,qu)\n",
    "            ind = i\n",
    "    closest_document = corpus[ind]\n",
    "    #### make use of cos_distance function above!\n",
    "    return closest_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I LIKE GREEN EGGS AND HAM'"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"I LIKE EGGS\"\n",
    "find_the_closest_document(corpus, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 (a) \n",
    "Now instead of using the raw counts, use TF-IDF for each entry in the term-document matrix. Using the vector space model and cosine similarity find the closest document to the user query “I LIKE EGGS” for the new index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_closest_document_tfidf(corpus, query: str)->str:\n",
    "    \"\"\" Find the closest document with query from corpus using tf-idf\n",
    "\n",
    "    Arguments:\n",
    "    corpus-- list of sentences in the corpus\n",
    "    query-- a string to compute cosine similarity with\n",
    "\n",
    "    Return:\n",
    "    closest_document --the string in the corpus which is the closest to the query\n",
    "    \"\"\"\n",
    "    closest_document=str()\n",
    "    #### YOUR CODE HERE ####\n",
    "    #### NOTE the base of log is 2, not 10 or e\n",
    "    docs = corpus[:] + [query]\n",
    "    vocab = list(set(w for doc in docs for w in word_tokenize(doc)))\n",
    "    token_in_docs = list(word_tokenize(doc) for doc in docs )\n",
    "    vocab.sort()\n",
    "    vocab_to_idx = {vocab:i for i,vocab in enumerate(vocab)}\n",
    "    tf_ = np.zeros((len(docs), len(vocab)))\n",
    "    idf_ = tf_.copy() \n",
    "    for i, tokens in enumerate(token_in_docs):\n",
    "        for token in tokens:\n",
    "            tf_[i][vocab_to_idx[token]] += 1\n",
    "    df_ = np.sum(tf_>0,axis=0)\n",
    "    idf_ = np.log2(\n",
    "        len(docs) / df_\n",
    "        )\n",
    "    tf_idf = tf_*idf_\n",
    "\n",
    "    score = 0\n",
    "    idx = 0\n",
    "    corpus_vecs = list(tf_idf[:len(docs)-1])\n",
    "    query_vec = np.tile(\n",
    "        tf_idf[len(docs)-1:],(len(docs)-1,1)\n",
    "    )\n",
    "    for i, (cor,qu) in enumerate(zip(corpus_vecs,query_vec)):\n",
    "        if( cos_distance(cor, qu) > score):\n",
    "            score = cos_distance(cor,qu)\n",
    "            idx = i\n",
    "    closest_document = docs[i]\n",
    "    return closest_document\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.10794633570596113\n",
      "0.2983191769122352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I LIKE GREEN EGGS AND HAM'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_the_closest_document_tfidf(corpus, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 (c)\n",
    "nstead of using cosine similarity we could also use the L2 distance. Which similarity function (L2 or cosine) would work better here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_closest_document_L2(corpus, query: str)->str:\n",
    "    \"\"\" Find the closest document with query from corpus using L2 distance\n",
    "\n",
    "    Arguments:\n",
    "    corpus-- list of sentences in the corpus\n",
    "    query-- a string to compute cosine similarity with\n",
    "\n",
    "    Return:\n",
    "    closest_document --the string in the corpus which is the closest to the query\n",
    "    \"\"\"\n",
    "    closest_document=str()\n",
    "    docs = corpus[:] + [query]\n",
    "    vocab = list(set(w for doc in docs for w in word_tokenize(doc)))\n",
    "    token_in_docs = list(word_tokenize(doc) for doc in docs )\n",
    "    vocab.sort()\n",
    "    vocab_to_idx = {vocab:i for i,vocab in enumerate(vocab)}\n",
    "    tf_ = np.zeros((len(docs), len(vocab)))\n",
    "    idf_ = tf_.copy() \n",
    "    for i, tokens in enumerate(token_in_docs):\n",
    "        for token in tokens:\n",
    "            tf_[i][vocab_to_idx[token]] += 1\n",
    "    df_ = np.sum(tf_>0,axis=0)\n",
    "    idf_ = np.log2(\n",
    "        len(docs) / df_\n",
    "        )\n",
    "    tf_idf = tf_*idf_\n",
    "\n",
    "    corpus_vecs = tf_idf[:len(docs)-1]\n",
    "    query_vec = np.tile(\n",
    "        tf_idf[len(docs)-1:],(len(docs)-1,1)\n",
    "    )\n",
    "    scores = np.sqrt(\n",
    "        np.sum(\n",
    "            np.power(\n",
    "                corpus_vecs - query_vec, 2\n",
    "            ),axis=1\n",
    "        )\n",
    "    )\n",
    "    idx = np.argmax(scores)\n",
    "    closest_document = corpus[idx]\n",
    "    #### YOUR CODE HERE ####\n",
    "    \n",
    "    return closest_document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.        0.        0.        0.        0.        0.        0.\n",
      "  1.       ]\n",
      " [1.        0.        0.        0.        0.        0.        0.4150375\n",
      "  1.       ]\n",
      " [0.        2.        1.        2.        2.        0.        0.4150375\n",
      "  0.       ]]\n",
      "[1.78108285 1.73205081 3.46410162]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I LIKE GREEN EGGS AND HAM'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "find_the_closest_document_L2(corpus, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('a1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec8c32ee267f0b77247536f66c6f437ef243017aefa1ed22759fece47f4c7d07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
